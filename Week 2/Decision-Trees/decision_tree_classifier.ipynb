{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "COUNT_THRESHOLD = 5  # Count threshold for labeling a node as a leaf\n",
    "MAX_DEPTH = 3  # Maximum depth for the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Calculate Gini Impurity\n",
    "def calculate_gini(data):\n",
    "    \"\"\"\n",
    "    Calculates the Gini impurity for a given dataset.\n",
    "\n",
    "    Gini Impurity is a metric used to measure how \"pure\" a dataset is.\n",
    "    - A pure dataset (all labels are the same) has Gini = 0.\n",
    "    - A completely heterogeneous dataset has Gini close to 1.\n",
    "\n",
    "    Formula:\n",
    "        Gini = 1 - Σ(p_i^2)  for all labels i\n",
    "        where p_i is the proportion of rows with label i.\n",
    "\n",
    "    Args:\n",
    "        data: A list of rows. Each row is a list, and the last element is the target label.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the Gini impurity.\n",
    "    \"\"\"\n",
    "    # Hint: Count the occurrences of each label in the dataset.\n",
    "    # Hint: Compute the probability of each label and use the formula for Gini impurity.\n",
    "    \n",
    "    # TODO: Implement the Gini impurity formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 2: Split dataset by an attribute\n",
    "def split_data(data, attribute_index, value):\n",
    "    \"\"\"\n",
    "    Splits the dataset into a subset based on the value of a specified attribute.\n",
    "\n",
    "    Args:\n",
    "        data: A list of rows. Each row is a list of attribute values.\n",
    "        attribute_index: The index of the attribute to split on.\n",
    "        value: The value of the attribute to match.\n",
    "\n",
    "    Returns:\n",
    "        A subset of the dataset where rows have the attribute at attribute_index equal to value.\n",
    "    \"\"\"\n",
    "    # Hint: Use a list comprehension to filter rows based on the attribute value.\n",
    "    # Hint: Remember to only include rows where data[attribute_index] == value.\n",
    "    \n",
    "    # TODO: Filter the rows based on the attribute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 3: Calculate Gini Gain\n",
    "def gini_gain(data, attribute_index):\n",
    "    \"\"\"\n",
    "    Calculates the Gini gain for a specific attribute.\n",
    "\n",
    "    Gini Gain measures how much Gini impurity decreases when splitting the dataset\n",
    "    on a specific attribute. The higher the gain, the better the attribute is for splitting.\n",
    "\n",
    "    Formula:\n",
    "        Gini Gain = Gini(parent) - Σ(weighted Gini(child subsets))\n",
    "\n",
    "    Args:\n",
    "        data: A list of rows. Each row is a list, and the last element is the target label.\n",
    "        attribute_index: The index of the attribute to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the Gini gain for the attribute.\n",
    "    \"\"\"\n",
    "    # Hint: Use calculate_gini() to compute the Gini impurity of the parent dataset.\n",
    "    # Hint: Use split_data() to create subsets for each unique value of the attribute.\n",
    "    # Hint: Compute the weighted Gini impurity for the subsets.\n",
    "    \n",
    "    # TODO: Implement the Gini gain formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 4: Build the Decision Tree\n",
    "def build_tree(data, attributes, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively builds a decision tree using Gini impurity to find the best splits.\n",
    "\n",
    "    Stopping conditions:\n",
    "    1. No attributes left to split on.\n",
    "    2. Depth exceeds MAX_DEPTH.\n",
    "    3. Number of elements in a leaf is less than COUNT_THRESHOLD.\n",
    "\n",
    "    Args:\n",
    "        data: A list of rows. Each row is a list, and the last element is the target label.\n",
    "        attributes: A list of indices representing the available attributes for splitting.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the decision tree.\n",
    "    \"\"\"\n",
    "    # Hint: Use calculate_gini() to check if the dataset is pure.\n",
    "    # Hint: If no attributes remain, return the majority label in the dataset.\n",
    "    # Hint: Use gini_gain() to find the best attribute for splitting.\n",
    "    \n",
    "    # This is what a decsision tree should look like\n",
    "    # {2: {'Single': {3: {'Low': 'Yes', 'Medium': 'No', 'High': {1: {'Low': 'Yes', 'High': 'No'}}}}, 'Married': 'Yes', 'Divorced': 'No'}}\n",
    "    \n",
    "    # TODO: Handle stopping conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 5: Predict a Single Instance\n",
    "def predict(tree, instance):\n",
    "    \"\"\"\n",
    "    Predicts the label for a single instance using the decision tree.\n",
    "\n",
    "    Args:\n",
    "        tree: The decision tree (dictionary).\n",
    "        instance: A dictionary where keys are attribute indices and values are the instance's attribute values.\n",
    "\n",
    "    Returns:\n",
    "        The predicted label for the instance.\n",
    "    \"\"\"\n",
    "    # Hint: Traverse the tree based on the instance's attribute values.\n",
    "    # Hint: If the current node is a dictionary, look up the next branch using the instance value.\n",
    "    # Hint: If the current node is not a dictionary, return it as the predicted label.\n",
    "    \n",
    "    # TODO: Implement tree traversal logic for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the CSV file\n",
    "df = pd.read_csv(\"car_evaluation.csv\")\n",
    "\n",
    "# Define a mapping for the categorical values to numerical values\n",
    "value_mappings = {\n",
    "    'buying': {'vhigh': 3, 'high': 2, 'med': 1, 'low': 0},\n",
    "    'maint': {'vhigh': 3, 'high': 2, 'med': 1, 'low': 0},\n",
    "    'doors': {'2': 0, '3': 1, '4': 2, '5more': 3},\n",
    "    'persons': {'2': 0, '4': 1, 'more': 2},\n",
    "    'lug_boot': {'small': 0, 'med': 1, 'big': 2},\n",
    "    'safety': {'low': 0, 'med': 1, 'high': 2}\n",
    "}\n",
    "\n",
    "# Apply the mappings to the dataset\n",
    "for column, mapping in value_mappings.items():\n",
    "    df[column] = df[column].map(mapping)\n",
    "\n",
    "# Extract the features and target\n",
    "X = df.drop(columns=['class'])  # Drop the target column\n",
    "y = df['class'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})  # Map target to numerical values\n",
    "\n",
    "# Convert DataFrame to list for the decision tree\n",
    "data = df.values.tolist()\n",
    "attributes = list(range(len(X.columns)))  # Indexes of the attributes (buying, maint, etc.)\n",
    "attribute_names = list(X.columns)  # Get the actual attribute names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decision tree\n",
    "tree = build_tree(data, attributes)\n",
    "print(\"Decision Tree:\", tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the decision tree\n",
    "def plot_tree(tree, depth=0, x_offset=0.5, y_offset=1.0, x_gap=0.25, ax=None, data=None, attribute_names=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.axis('off')  # Turn off the axes\n",
    "\n",
    "    # If the tree is a leaf node, plot the label and counts\n",
    "    if not isinstance(tree, dict):\n",
    "        # Count the occurrences of each label in the current leaf data subset\n",
    "        label_counts = {label: sum(1 for row in data if row[-1] == label) for label in set(row[-1] for row in data)}\n",
    "        # Format the label text to include counts of each class\n",
    "        label_text = f\"{tree}\\n\" + \"\\n\".join([f\"{label}: {count}\" for label, count in label_counts.items()])\n",
    "        ax.text(x_offset, y_offset, label_text, fontsize=10, ha='center', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", edgecolor=\"black\"))\n",
    "        return\n",
    "\n",
    "    # Get the current decision attribute name\n",
    "    root = list(tree.keys())[0]\n",
    "    attribute_name = attribute_names[root]  # Map the index to the attribute name\n",
    "    ax.text(x_offset, y_offset, f\"{attribute_name}\", fontsize=12, ha='center',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", edgecolor=\"black\"))\n",
    "\n",
    "    # Recursively plot child nodes\n",
    "    num_children = len(tree[root])\n",
    "    for i, (value, subtree) in enumerate(tree[root].items()):\n",
    "        child_x = x_offset + (i - (num_children - 1) / 2) * x_gap  # Calculate child x position\n",
    "        child_y = y_offset - 0.1  # Shift vertically for the child nodes\n",
    "\n",
    "        # Draw a line to the child node\n",
    "        ax.plot([x_offset, child_x], [y_offset - 0.02, child_y + 0.02], 'k-', lw=1)\n",
    "\n",
    "        # Recursively call plot_tree for the child node, passing the subset of data that corresponds to the child\n",
    "        subset = split_data(data, root, value)  # Get the subset of data for this child\n",
    "        plot_tree(subtree, depth + 1, child_x, child_y, x_gap / 2, ax=ax, data=subset, attribute_names=attribute_names)\n",
    "\n",
    "    if depth == 0:  # Show the plot only at the top level\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_tree(tree, data=data, attribute_names=attribute_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "test_instance = {0: 1, 1: 2}  # Middle-aged with High income\n",
    "prediction = predict(tree, test_instance)\n",
    "print(\"\\nPrediction for test instance:\", \"Yes\" if prediction == 1 else \"No\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
